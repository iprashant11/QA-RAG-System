# QA-RAG-System
This is the QA system based on hugging face and llama3 model.
Goal: Answer a Question Using PDF Knowledge
You're using:
ï‚· ğŸ“š Pre-indexed PDF chunks with FAISS
ï‚· ğŸ¤– LLaMA3 (via Ollama) as the answering engine
ï‚· ğŸ” Hugging Face embedding model for semantic search
ğŸ” Query Time Process: In-Depth Steps
ğŸŸ© Step 1: User Asks a Question
ğŸ“¨
Input Example:
"What are the objectives of the NDPS Act?"
ğŸ” This question will be processed through the following pipeline:
ğŸŸ© Step 2: Convert Question into Vector (Embedding)
sentence-transformers/all-MiniLM-L6-v2
This converts:
"What are the objectives of the NDPS Act?"
Into a dense vector like:
[0.21, -0.44, ..., 0.12] (768-dimensional)
ğŸ‘‰ This vector captures the meaning of the sentence â€” not just the words.
ğŸŸ¨ Step 3: Retrieve Top-k Similar Chunks from Vector DB
Now you query FAISS:
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
ğŸ“¦ FAISS searches for the 5 most similar chunks based on cosine similarity between:
ï‚· the question vector
ï‚· the stored PDF chunk vectors
ğŸŸ¨ Example Match (Chunk from PDF):
"The NDPS Act aims to control and regulate operations relating to narcotic drugs and psychotropic
substances, to prevent their misuse, and to implement international treaties."
This chunk has high semantic similarity to the userâ€™s question, even if the exact words donâ€™t match.
ğŸŸ¨ Step 4: Send Retrieved Chunks + Question to LLM (LLaMA3)
Now you form a prompt like:
Context:
"The NDPS Act aims to control and regulate operations relating to narcotic drugs and psychotropic
substances..."
Question:
"What are the objectives of the NDPS Act?"
Answer:
ğŸ” This prompt is passed to Ollama (LLaMA3) through the RetrievalQA chain:
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
result = qa_chain.run(question)
ğŸŸ¨ Step 5: LLM Generates a Final Answer
Using the context, LLaMA3 generates:
"The NDPS Act primarily aims to regulate and control narcotic drugs and psychotropic
substances, prevent their misuse, and fulfill international treaty obligations."
âœ… This answer is not just copied â€” itâ€™s abstractive generation:
LLM understands and rephrases the context into a human-friendly answer.
ğŸŸ¨ Step 6: Show Answer to User (via Streamlit)
In your Streamlit app:
st.write("Answer:", result)
Optional:
Also show the source chunk to build user trust:
return_source_documents=True
ğŸ”„ Summary of Flow (Visual Representation)
[User Question]
â†“
[Hugging Face Embedding (MiniLM)]
â†“
[FAISS Vector DB â†’ Top-k Relevant Chunks]
â†“
[Prompt + Chunks â†’ LLaMA3 via Ollama]
â†“
[LLM-Generated Answer]
â†“
[UI Display in Streamlit]
ğŸŸ¨ Why This Is Powerful
ï‚· Combines factual accuracy (from documents) + natural language generation (LLM)
ï‚· Handles semantic questions, not just keyword matches
ï‚· Can work offline (great for sensitive or secure applications)
